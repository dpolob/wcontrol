# COSAS POR HACER

- kwargs_gen ¿porque es una clase? debiera ser funciones

    




            
- Fijarse en el optim para renombrar ciertas cosas del modelo num_layer en encoder por ejemplo
- Para el tema de bins hay mucho hardcode
- Cambiar nombre a modules (es mu feo)
- Crear un wrapper que integre la creacion de decoder y encoder solo pasando (Ft, Ff, Fnwp, Fout) 
No lo hago porque pierdo trazabilidad pero es buena técnica. Para hacer cuando optimice el codigo
final

- La salida X en el dataset no vale para nada si funciona 3salidas y aun asi creo que tampoco.
Efectivamente se puede eliminar porque no se usa X para nada y solo consume recursos, pero...
cuesta mucho eliminarlo y solo lo haré cuando optimice el codigo final.

- Hacer bins dinamicos. Solo si tengo mucho mucho tiempo



class Seq2SeqDataset(Dataset):
    """
    
    REHACER ENTERO
    etiquetaX es la prediccion 8temperatura, hr, precipitacion
    etiquetaP es la del proveedor de NWP
    etiquetaF son las features
    etiquetaT son las temporales




# COSAS HECHAS
- clase abstracta de BaseTrainer
- CAMBIAR: 
    def _early_stopping(self, epoch_without_resuming: int) -> bool:
        """ Devuelve True si se cumplen las condiciones de early stopping
        ultimo epoch - mejor_epoch > valor definido en self.early_stop"""
        last_epoch_index = len(self.valid_losses)
                
        if last_epoch_index > self.early_stop and epoch_without_resuming > self.early_stop:
            best_epoch_index = sorted(self.valid_losses.items(), key=lambda x:x[1])[0][0]
            if (last_epoch_index - best_epoch_index) > self.early_stop:
                return True
        return False
- keep_best_checkpoint = True Añadir para eliminar el resto de checkpoint que no valen

- Quitar hparams
- precipitacion acumulada. No ha valido
- imputacion de resultados para NaNs (tengo muchos). No ha valido
- Usar mape. Da muchos problemas para valores proximos a 0, mejor uso L1

- corregir maxMSE y maxL1 porque no solo sacaban en máximo entre todas las series. Ahora se hace media
- MAX(MEAN(error(reduction=none)))

- Quitar las variables de tiempo relativas al dia de la semana (no hay estacionalidad) por lo que no pintan nada
- Bins para crear los weights de crossentropyloss
0    150541
1      1621
2       512
3       249
5       176
4       121
6        46
7         1

- tqdm en escalado del generarvariablesZmodel
- seq2seq No hardcoding de la salida del decoder (N,1) -> (N, Fout)
- Revisar esto en trainer._loss_batch       
´´´if pass_y:
    y_pred = self.model(Xt, X, Yt, Y, P, teacher)
else:
    y_pred = self.model(Xt, X, Yt, Y, P, teacher)´´´
- Quitar todo lo de teacher forcing y duplicate teacher forcing
- Salvar hyparams. Se guardan en el modelo con torch.save. Antes lo hacia mal


- Torch manual seed y numpy seed poner al principio del script Train y Predict
