# COSAS POR HACER

from os import listxattr


- Quitar hparams


- Esto hay que externalizarlo y devolver una lista de muestras 
    ##############################################
    #### COSAS QUE HACER AQUI               ######
    ##############################################
    
    if isinstance(cfg.zmodel.resultados.visualizacion.muestras, str):  # all, random o range
        if cfg.zmodel.resultados.visualizacion.muestras == 'range':  # range
            inicio = cfg.zmodel.resultados.visualizacion.inicio
            fin = len(y_pred) if cfg.zmodel.resultados.visualizacion.fin == 'end' else cfg.zmodel.resultados.visualizacion.fin
            paso = cfg.zmodel.resultados.visualizacion.paso
            muestras = range(inicio, fin, paso)
    

- keep_best_checkpoint = True Añadir para eliminar el resto de checkpoint que no valen


- CAMBIAR: 
    def _early_stopping(self, epoch_without_resuming: int) -> bool:
        """ Devuelve True si se cumplen las condiciones de early stopping
        ultimo epoch - mejor_epoch > valor definido en self.early_stop"""
        last_epoch_index = len(self.valid_losses)
                
        if last_epoch_index > self.early_stop and epoch_without_resuming > self.early_stop:
            best_epoch_index = sorted(self.valid_losses.items(), key=lambda x:x[1])[0][0]
            if (last_epoch_index - best_epoch_index) > self.early_stop:
                return True
        return False
            
- Fijarse en el optim para renombrar ciertas cosas del modelo num_layer en encoder por ejemplo
- Para el tema de bins hay mucho hardcode
- Cambiar nombre a modules (es mu feo)
- Guardar hiperparametros con el modelo, para no tener que crear los Encoders y Decoders en el predict

- Crear un wrapper que integre la creacion de decoder y encoder solo pasando (Ft, Ff, Fnwp, Fout) 
No lo hago porque pierdo trazabilidad pero es buena técnica. Para hacer cuando optimice el codigo
final

- La salida X en el dataset no vale para nada si funciona 3salidas y aun asi creo que tampoco.
Efectivamente se puede eliminar porque no se usa X para nada y solo consume recursos, pero...
cuesta mucho eliminarlo y solo lo haré cuando optimice el codigo final.

- Hacer bins dinamicos
Solo si tengo mucho mucho tiempo

- Torch manual seed y numpy seed poner al principio del script Train y Predict


class Seq2SeqDataset(Dataset):
    """
    
    REHACER ENTERO
    etiquetaX es la prediccion 8temperatura, hr, precipitacion
    etiquetaP es la del proveedor de NWP
    etiquetaF son las features
    etiquetaT son las temporales




# COSAS HECHAS
- precipitacion acumulada. No ha valido
- imputacion de resultados para NaNs (tengo muchos). No ha valido
- Usar mape. Da muchos problemas para valores proximos a 0, mejor uso L1

- corregir maxMSE y maxL1 porque no solo sacaban en máximo entre todas las series. Ahora se hace media
- MAX(MEAN(error(reduction=none)))

- Quitar las variables de tiempo relativas al dia de la semana (no hay estacionalidad) por lo que no pintan nada
- Bins para crear los weights de crossentropyloss
0    150541
1      1621
2       512
3       249
5       176
4       121
6        46
7         1

- tqdm en escalado del generarvariablesZmodel
- seq2seq No hardcoding de la salida del decoder (N,1) -> (N, Fout)
- Revisar esto en trainer._loss_batch       
´´´if pass_y:
    y_pred = self.model(Xt, X, Yt, Y, P, teacher)
else:
    y_pred = self.model(Xt, X, Yt, Y, P, teacher)´´´
- Quitar todo lo de teacher forcing y duplicate teacher forcing
- Salvar hyparams. Se guardan en el modelo con torch.save. Antes lo hacia mal